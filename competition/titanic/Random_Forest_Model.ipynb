{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-31T23:57:37.236414Z",
     "start_time": "2025-08-31T23:57:37.233059Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from itertools import product"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T23:59:51.140759Z",
     "start_time": "2025-08-31T23:59:51.045478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"data/train.csv\", delimiter=\",\").drop(columns=[\"Name\", \"Ticket\"])\n",
    "cat_variables = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "le = OneHotEncoder(handle_unknown= \"ignore\", sparse_output=False)\n",
    "df[\"Cabin\"] = le.fit_transform(df[\"Cabin\"])\n",
    "df[\"Embarked\"] = le.fit_transform(df[\"Embarked\"])\n",
    "df = pd.get_dummies(df, prefix=cat_variables, columns=cat_variables)\n",
    "features = [x for x in df.columns if x != \"Survived\"]\n",
    "x, y = df[features], df[\"Survived\"].T\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ],
   "id": "bb48712c863d68f2",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m cat_variables = [\u001B[33m\"\u001B[39m\u001B[33mPclass\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mSex\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mSibSp\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mParch\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m      3\u001B[39m le = OneHotEncoder(handle_unknown= \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m, sparse_output=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m df[\u001B[33m\"\u001B[39m\u001B[33mCabin\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mle\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mCabin\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m df[\u001B[33m\"\u001B[39m\u001B[33mEmbarked\u001B[39m\u001B[33m\"\u001B[39m] = le.fit_transform(df[\u001B[33m\"\u001B[39m\u001B[33mEmbarked\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m      6\u001B[39m df = pd.get_dummies(df, prefix=cat_variables, columns=cat_variables)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    318\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    319\u001B[39m         return_tuple = (\n\u001B[32m    320\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    321\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    322\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/base.py:894\u001B[39m, in \u001B[36mTransformerMixin.fit_transform\u001B[39m\u001B[34m(self, X, y, **fit_params)\u001B[39m\n\u001B[32m    879\u001B[39m         warnings.warn(\n\u001B[32m    880\u001B[39m             (\n\u001B[32m    881\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThis object (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) has a `transform`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    889\u001B[39m             \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[32m    890\u001B[39m         )\n\u001B[32m    892\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    893\u001B[39m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m894\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m.transform(X)\n\u001B[32m    895\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    896\u001B[39m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[32m    897\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:991\u001B[39m, in \u001B[36mOneHotEncoder.fit\u001B[39m\u001B[34m(self, X, y)\u001B[39m\n\u001B[32m    972\u001B[39m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    973\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    974\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    975\u001B[39m \u001B[33;03m    Fit OneHotEncoder to X.\u001B[39;00m\n\u001B[32m    976\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    989\u001B[39m \u001B[33;03m        Fitted encoder.\u001B[39;00m\n\u001B[32m    990\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m991\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    992\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    993\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhandle_unknown\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle_unknown\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    994\u001B[39m \u001B[43m        \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    995\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    996\u001B[39m     \u001B[38;5;28mself\u001B[39m._set_drop_idx()\n\u001B[32m    997\u001B[39m     \u001B[38;5;28mself\u001B[39m._n_features_outs = \u001B[38;5;28mself\u001B[39m._compute_n_features_outs()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:83\u001B[39m, in \u001B[36m_BaseEncoder._fit\u001B[39m\u001B[34m(self, X, handle_unknown, ensure_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001B[39m\n\u001B[32m     81\u001B[39m _check_n_features(\u001B[38;5;28mself\u001B[39m, X, reset=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     82\u001B[39m _check_feature_names(\u001B[38;5;28mself\u001B[39m, X, reset=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m83\u001B[39m X_list, n_samples, n_features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_check_X\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     84\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_all_finite\u001B[49m\n\u001B[32m     85\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     86\u001B[39m \u001B[38;5;28mself\u001B[39m.n_features_in_ = n_features\n\u001B[32m     88\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.categories != \u001B[33m\"\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:49\u001B[39m, in \u001B[36m_BaseEncoder._check_X\u001B[39m\u001B[34m(self, X, ensure_all_finite)\u001B[39m\n\u001B[32m     36\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     37\u001B[39m \u001B[33;03mPerform custom check_array:\u001B[39;00m\n\u001B[32m     38\u001B[39m \u001B[33;03m- convert list of strings to object dtype\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     45\u001B[39m \n\u001B[32m     46\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mhasattr\u001B[39m(X, \u001B[33m\"\u001B[39m\u001B[33miloc\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(X, \u001B[33m\"\u001B[39m\u001B[33mndim\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m0\u001B[39m) == \u001B[32m2\u001B[39m):\n\u001B[32m     48\u001B[39m     \u001B[38;5;66;03m# if not a dataframe, do normal check_array validation\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m     X_temp = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[33m\"\u001B[39m\u001B[33mdtype\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m np.issubdtype(X_temp.dtype, np.str_):\n\u001B[32m     51\u001B[39m         X = check_array(X, dtype=\u001B[38;5;28mobject\u001B[39m, ensure_all_finite=ensure_all_finite)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1091\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1084\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1085\u001B[39m             msg = (\n\u001B[32m   1086\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mExpected 2D array, got 1D array instead:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33marray=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00marray\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1087\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mReshape your data either using array.reshape(-1, 1) if \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1088\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33myour data has a single feature or array.reshape(1, -1) \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1089\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mif it contains a single sample.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1090\u001B[39m             )\n\u001B[32m-> \u001B[39m\u001B[32m1091\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m   1093\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m dtype_numeric \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(array.dtype, \u001B[33m\"\u001B[39m\u001B[33mkind\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m array.dtype.kind \u001B[38;5;129;01min\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mUSV\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1094\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1095\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mdtype=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mnumeric\u001B[39m\u001B[33m'\u001B[39m\u001B[33m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1096\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1097\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead."
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T23:42:57.471955Z",
     "start_time": "2025-08-31T23:42:57.466331Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_cv, y_train, y_cv = train_test_split(x, y, train_size=0.8, random_state=1)",
   "id": "78f301c3491e3f22",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T23:42:58.164377Z",
     "start_time": "2025-08-31T23:42:58.160258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"X train: {X_train.shape}\")\n",
    "print(f\"Y train: {y_train.shape}\")\n",
    "print(f\"X cv: {X_cv.shape}\")\n",
    "print(f\"y cv: {y_cv.shape}\")\n"
   ],
   "id": "b9c74300ffcb5360",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (712, 24)\n",
      "Y train: (712,)\n",
      "X cv: (179, 24)\n",
      "y cv: (179,)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T23:42:59.112472Z",
     "start_time": "2025-08-31T23:42:59.110617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_depth_list = [2, 3, 4, 5, 6, 7, 8]\n",
    "min_samples_split_list = [30, 50, 100, 150, 200, 250, 300]\n",
    "n_estimators_list = [50, 100, 150, 200]"
   ],
   "id": "8825c4c77a69a762",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T23:48:49.676784Z",
     "start_time": "2025-08-31T23:48:36.803504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_models(X_train, y_train, X_cv, y_cv, max_depth, min_samples_split, n_estimators):\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth,\n",
    "                                   min_samples_split=min_samples_split,\n",
    "                                   random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_cv_hat = model.predict(X_cv)\n",
    "    cv_error = np.mean(y_cv != y_cv_hat)\n",
    "    print(f\"Max depth: {max_depth}, Min sample split: {min_samples_split}, N estimators: {n_estimators}, Error: {cv_error}\")\n",
    "    return model, max_depth, min_samples_split, n_estimators, cv_error\n",
    "\n",
    "\n",
    "best_model_params = min([eval_models(X_train, y_train, X_cv, y_cv, max_depth, min_samples_split, n_estimators) for\n",
    "                         max_depth, min_samples_split, n_estimators in\n",
    "                         product(max_depth_list, min_samples_split_list, n_estimators_list)], key=lambda x: x[4])[:4]\n",
    "print(best_model_params[-3:])"
   ],
   "id": "6c0ddb1fcff877c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth: 2, Min sample split: 30, N estimators: 50, Error: 0.2737430167597765\n",
      "Max depth: 2, Min sample split: 30, N estimators: 100, Error: 0.25139664804469275\n",
      "Max depth: 2, Min sample split: 30, N estimators: 150, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 30, N estimators: 200, Error: 0.2569832402234637\n",
      "Max depth: 2, Min sample split: 50, N estimators: 50, Error: 0.2681564245810056\n",
      "Max depth: 2, Min sample split: 50, N estimators: 100, Error: 0.25139664804469275\n",
      "Max depth: 2, Min sample split: 50, N estimators: 150, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 50, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 100, N estimators: 50, Error: 0.2681564245810056\n",
      "Max depth: 2, Min sample split: 100, N estimators: 100, Error: 0.25139664804469275\n",
      "Max depth: 2, Min sample split: 100, N estimators: 150, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 100, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 150, N estimators: 50, Error: 0.2681564245810056\n",
      "Max depth: 2, Min sample split: 150, N estimators: 100, Error: 0.24581005586592178\n",
      "Max depth: 2, Min sample split: 150, N estimators: 150, Error: 0.25139664804469275\n",
      "Max depth: 2, Min sample split: 150, N estimators: 200, Error: 0.25139664804469275\n",
      "Max depth: 2, Min sample split: 200, N estimators: 50, Error: 0.25139664804469275\n",
      "Max depth: 2, Min sample split: 200, N estimators: 100, Error: 0.25139664804469275\n",
      "Max depth: 2, Min sample split: 200, N estimators: 150, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 200, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 250, N estimators: 50, Error: 0.2569832402234637\n",
      "Max depth: 2, Min sample split: 250, N estimators: 100, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 250, N estimators: 150, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 250, N estimators: 200, Error: 0.2681564245810056\n",
      "Max depth: 2, Min sample split: 300, N estimators: 50, Error: 0.2569832402234637\n",
      "Max depth: 2, Min sample split: 300, N estimators: 100, Error: 0.2569832402234637\n",
      "Max depth: 2, Min sample split: 300, N estimators: 150, Error: 0.26256983240223464\n",
      "Max depth: 2, Min sample split: 300, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 3, Min sample split: 30, N estimators: 50, Error: 0.21787709497206703\n",
      "Max depth: 3, Min sample split: 30, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 3, Min sample split: 30, N estimators: 150, Error: 0.22905027932960895\n",
      "Max depth: 3, Min sample split: 30, N estimators: 200, Error: 0.21787709497206703\n",
      "Max depth: 3, Min sample split: 50, N estimators: 50, Error: 0.21787709497206703\n",
      "Max depth: 3, Min sample split: 50, N estimators: 100, Error: 0.22905027932960895\n",
      "Max depth: 3, Min sample split: 50, N estimators: 150, Error: 0.2346368715083799\n",
      "Max depth: 3, Min sample split: 50, N estimators: 200, Error: 0.24022346368715083\n",
      "Max depth: 3, Min sample split: 100, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 3, Min sample split: 100, N estimators: 100, Error: 0.24022346368715083\n",
      "Max depth: 3, Min sample split: 100, N estimators: 150, Error: 0.2346368715083799\n",
      "Max depth: 3, Min sample split: 100, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 3, Min sample split: 150, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 3, Min sample split: 150, N estimators: 100, Error: 0.25139664804469275\n",
      "Max depth: 3, Min sample split: 150, N estimators: 150, Error: 0.24022346368715083\n",
      "Max depth: 3, Min sample split: 150, N estimators: 200, Error: 0.2346368715083799\n",
      "Max depth: 3, Min sample split: 200, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 3, Min sample split: 200, N estimators: 100, Error: 0.21787709497206703\n",
      "Max depth: 3, Min sample split: 200, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 3, Min sample split: 200, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 3, Min sample split: 250, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 3, Min sample split: 250, N estimators: 100, Error: 0.24581005586592178\n",
      "Max depth: 3, Min sample split: 250, N estimators: 150, Error: 0.2346368715083799\n",
      "Max depth: 3, Min sample split: 250, N estimators: 200, Error: 0.24022346368715083\n",
      "Max depth: 3, Min sample split: 300, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 3, Min sample split: 300, N estimators: 100, Error: 0.2681564245810056\n",
      "Max depth: 3, Min sample split: 300, N estimators: 150, Error: 0.25139664804469275\n",
      "Max depth: 3, Min sample split: 300, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 4, Min sample split: 30, N estimators: 50, Error: 0.2122905027932961\n",
      "Max depth: 4, Min sample split: 30, N estimators: 100, Error: 0.18994413407821228\n",
      "Max depth: 4, Min sample split: 30, N estimators: 150, Error: 0.20670391061452514\n",
      "Max depth: 4, Min sample split: 30, N estimators: 200, Error: 0.21787709497206703\n",
      "Max depth: 4, Min sample split: 50, N estimators: 50, Error: 0.18994413407821228\n",
      "Max depth: 4, Min sample split: 50, N estimators: 100, Error: 0.21787709497206703\n",
      "Max depth: 4, Min sample split: 50, N estimators: 150, Error: 0.20670391061452514\n",
      "Max depth: 4, Min sample split: 50, N estimators: 200, Error: 0.21787709497206703\n",
      "Max depth: 4, Min sample split: 100, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 100, N estimators: 100, Error: 0.24022346368715083\n",
      "Max depth: 4, Min sample split: 100, N estimators: 150, Error: 0.24022346368715083\n",
      "Max depth: 4, Min sample split: 100, N estimators: 200, Error: 0.2346368715083799\n",
      "Max depth: 4, Min sample split: 150, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 150, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 150, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 150, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 200, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 200, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 200, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 200, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 250, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 250, N estimators: 100, Error: 0.2346368715083799\n",
      "Max depth: 4, Min sample split: 250, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 250, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 300, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 4, Min sample split: 300, N estimators: 100, Error: 0.2681564245810056\n",
      "Max depth: 4, Min sample split: 300, N estimators: 150, Error: 0.25139664804469275\n",
      "Max depth: 4, Min sample split: 300, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 5, Min sample split: 30, N estimators: 50, Error: 0.2122905027932961\n",
      "Max depth: 5, Min sample split: 30, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 30, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 30, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 50, N estimators: 50, Error: 0.22905027932960895\n",
      "Max depth: 5, Min sample split: 50, N estimators: 100, Error: 0.22905027932960895\n",
      "Max depth: 5, Min sample split: 50, N estimators: 150, Error: 0.2346368715083799\n",
      "Max depth: 5, Min sample split: 50, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 100, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 100, N estimators: 100, Error: 0.22905027932960895\n",
      "Max depth: 5, Min sample split: 100, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 100, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 150, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 150, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 150, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 150, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 5, Min sample split: 200, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 200, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 200, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 200, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 5, Min sample split: 250, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 250, N estimators: 100, Error: 0.2346368715083799\n",
      "Max depth: 5, Min sample split: 250, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 250, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 300, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 5, Min sample split: 300, N estimators: 100, Error: 0.2681564245810056\n",
      "Max depth: 5, Min sample split: 300, N estimators: 150, Error: 0.25139664804469275\n",
      "Max depth: 5, Min sample split: 300, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 6, Min sample split: 30, N estimators: 50, Error: 0.21787709497206703\n",
      "Max depth: 6, Min sample split: 30, N estimators: 100, Error: 0.21787709497206703\n",
      "Max depth: 6, Min sample split: 30, N estimators: 150, Error: 0.2122905027932961\n",
      "Max depth: 6, Min sample split: 30, N estimators: 200, Error: 0.2122905027932961\n",
      "Max depth: 6, Min sample split: 50, N estimators: 50, Error: 0.21787709497206703\n",
      "Max depth: 6, Min sample split: 50, N estimators: 100, Error: 0.22905027932960895\n",
      "Max depth: 6, Min sample split: 50, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 50, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 100, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 100, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 100, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 100, N estimators: 200, Error: 0.21787709497206703\n",
      "Max depth: 6, Min sample split: 150, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 150, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 150, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 150, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 6, Min sample split: 200, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 200, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 200, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 200, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 6, Min sample split: 250, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 250, N estimators: 100, Error: 0.2346368715083799\n",
      "Max depth: 6, Min sample split: 250, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 250, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 300, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 6, Min sample split: 300, N estimators: 100, Error: 0.2681564245810056\n",
      "Max depth: 6, Min sample split: 300, N estimators: 150, Error: 0.25139664804469275\n",
      "Max depth: 6, Min sample split: 300, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 7, Min sample split: 30, N estimators: 50, Error: 0.22905027932960895\n",
      "Max depth: 7, Min sample split: 30, N estimators: 100, Error: 0.21787709497206703\n",
      "Max depth: 7, Min sample split: 30, N estimators: 150, Error: 0.2122905027932961\n",
      "Max depth: 7, Min sample split: 30, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 50, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 50, N estimators: 100, Error: 0.2122905027932961\n",
      "Max depth: 7, Min sample split: 50, N estimators: 150, Error: 0.2122905027932961\n",
      "Max depth: 7, Min sample split: 50, N estimators: 200, Error: 0.21787709497206703\n",
      "Max depth: 7, Min sample split: 100, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 100, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 100, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 100, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 7, Min sample split: 150, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 150, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 150, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 150, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 7, Min sample split: 200, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 200, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 200, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 200, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 7, Min sample split: 250, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 250, N estimators: 100, Error: 0.2346368715083799\n",
      "Max depth: 7, Min sample split: 250, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 250, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 300, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 7, Min sample split: 300, N estimators: 100, Error: 0.2681564245810056\n",
      "Max depth: 7, Min sample split: 300, N estimators: 150, Error: 0.25139664804469275\n",
      "Max depth: 7, Min sample split: 300, N estimators: 200, Error: 0.26256983240223464\n",
      "Max depth: 8, Min sample split: 30, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 30, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 30, N estimators: 150, Error: 0.21787709497206703\n",
      "Max depth: 8, Min sample split: 30, N estimators: 200, Error: 0.21787709497206703\n",
      "Max depth: 8, Min sample split: 50, N estimators: 50, Error: 0.24022346368715083\n",
      "Max depth: 8, Min sample split: 50, N estimators: 100, Error: 0.21787709497206703\n",
      "Max depth: 8, Min sample split: 50, N estimators: 150, Error: 0.2346368715083799\n",
      "Max depth: 8, Min sample split: 50, N estimators: 200, Error: 0.24022346368715083\n",
      "Max depth: 8, Min sample split: 100, N estimators: 50, Error: 0.21787709497206703\n",
      "Max depth: 8, Min sample split: 100, N estimators: 100, Error: 0.21787709497206703\n",
      "Max depth: 8, Min sample split: 100, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 100, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 8, Min sample split: 150, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 150, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 150, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 150, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 8, Min sample split: 200, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 200, N estimators: 100, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 200, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 200, N estimators: 200, Error: 0.22905027932960895\n",
      "Max depth: 8, Min sample split: 250, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 250, N estimators: 100, Error: 0.2346368715083799\n",
      "Max depth: 8, Min sample split: 250, N estimators: 150, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 250, N estimators: 200, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 300, N estimators: 50, Error: 0.22346368715083798\n",
      "Max depth: 8, Min sample split: 300, N estimators: 100, Error: 0.2681564245810056\n",
      "Max depth: 8, Min sample split: 300, N estimators: 150, Error: 0.25139664804469275\n",
      "Max depth: 8, Min sample split: 300, N estimators: 200, Error: 0.26256983240223464\n",
      "(4, 30, 100)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T23:52:10.235408Z",
     "start_time": "2025-08-31T23:52:10.187050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test = pd.read_csv(\"data/test.csv\", delimiter=\",\").drop(columns=[\"Name\", \"Ticket\"])\n",
    "df_test[\"Cabin\"] = le.transform(df_test[\"Cabin\"])\n",
    "df_test[\"Embarked\"] = le.transform(df_test[\"Embarked\"])\n",
    "predictions = best_model_params[0].predict(df_test)\n",
    "output = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ],
   "id": "561abb7b1152d890",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'B45'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/utils/_encode.py:235\u001B[39m, in \u001B[36m_encode\u001B[39m\u001B[34m(values, uniques, check_unknown)\u001B[39m\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m235\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_map_to_integer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muniques\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    236\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/utils/_encode.py:174\u001B[39m, in \u001B[36m_map_to_integer\u001B[39m\u001B[34m(values, uniques)\u001B[39m\n\u001B[32m    173\u001B[39m table = _nandict({val: i \u001B[38;5;28;01mfor\u001B[39;00m i, val \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(uniques)})\n\u001B[32m--> \u001B[39m\u001B[32m174\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m xp.asarray([\u001B[43mtable\u001B[49m\u001B[43m[\u001B[49m\u001B[43mv\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m values], device=device(values))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/utils/_encode.py:167\u001B[39m, in \u001B[36m_nandict.__missing__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.nan_value\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'B45'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m df_test = pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33mdata/test.csv\u001B[39m\u001B[33m\"\u001B[39m, delimiter=\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m).drop(columns=[\u001B[33m\"\u001B[39m\u001B[33mName\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mTicket\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m df_test[\u001B[33m\"\u001B[39m\u001B[33mCabin\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mle\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_test\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mCabin\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m df_test[\u001B[33m\"\u001B[39m\u001B[33mEmbarked\u001B[39m\u001B[33m\"\u001B[39m] = le.transform(df_test[\u001B[33m\"\u001B[39m\u001B[33mEmbarked\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m      4\u001B[39m predictions = best_model_params[\u001B[32m0\u001B[39m].predict(df_test)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_label.py:134\u001B[39m, in \u001B[36mLabelEncoder.transform\u001B[39m\u001B[34m(self, y)\u001B[39m\n\u001B[32m    131\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _num_samples(y) == \u001B[32m0\u001B[39m:\n\u001B[32m    132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m xp.asarray([])\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muniques\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclasses_\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Project/perso/machine-learning/kaggle-comp/.venv/lib/python3.13/site-packages/sklearn/utils/_encode.py:237\u001B[39m, in \u001B[36m_encode\u001B[39m\u001B[34m(values, uniques, check_unknown)\u001B[39m\n\u001B[32m    235\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m _map_to_integer(values, uniques)\n\u001B[32m    236\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m237\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33my contains previously unseen labels: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    239\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m check_unknown:\n",
      "\u001B[31mValueError\u001B[39m: y contains previously unseen labels: 'B45'"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
